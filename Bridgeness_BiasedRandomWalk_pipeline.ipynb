{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import bridgeness\n",
    "from numpy.random import choice, uniform\n",
    "\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "import time\n",
    "import os\n",
    "\n",
    "from joblib import Memory\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.argv = ['foo'] #to make parseargs work in ipython too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--graph\", type=str, help=\"Graph filename\", default='graph2.txt.csv')\n",
    "parser.add_argument(\"--num_steps\", type=int, help=\"Number of steps\", default=10000)\n",
    "parser.add_argument(\"--check_every\" , type=int, help=\"Check for convergence every N steps\", default=100)\n",
    "parser.add_argument(\"--phi\" , type=float, help=\"Phi marameter\", default=100)\n",
    "parser.add_argument(\"--mu\" , type=float, help=\"Mu parameter\", default=0.2)\n",
    "parser.add_argument(\"--repeats\" , type=int, help=\"Number of parallel runs\", default=4)\n",
    "parser.add_argument(\"--conv_thr\" , type=float, help=\"Convergence threshold\", default=1.0e-6)\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = args.graph\n",
    "data = pd.read_csv(FN, delimiter = \" \")\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i,s in data.iterrows():\n",
    "    u = s[0]\n",
    "    v = s[1]\n",
    "\n",
    "    G.add_edge(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(cachedir='./cached_bri', verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def cached_bridgeness(G):\n",
    "    print(\"Computing centrality metric (cache miss), please be patient.\")\n",
    "    return bridgeness.bridgeness_centrality(G)\n",
    "\n",
    "bri = cached_bridgeness(G)\n",
    "#bri = bridgeness.bridgeness_centrality(G)\n",
    "#e_bri = bridgeness.edge_bridgeness_centrality(G)\n",
    "#bet = bridgeness.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = args.phi\n",
    "mu = args.mu\n",
    "#phi = 100\n",
    "#mu = 0.2\n",
    "\n",
    "#lab0 = 81  #82 in matlab\n",
    "#lab1 = 190 #191 in matlab\n",
    "labs = ((81, 0), (190, 1)) #seed nodes: community\n",
    "\n",
    "adj = nx.to_numpy_matrix(G)\n",
    "bri_vals = list(bri.values())\n",
    "quality = (1 / np.exp( np.multiply( bri_vals, phi )))\n",
    "diag = np.diagflat(quality) #k: v = node, bridgeness\n",
    "_tmp = np.matmul(diag, adj)\n",
    "_tmp_sum_on_rows = np.sum(_tmp, axis=0)\n",
    "_tmp_sum_on_rows_recip = np.reciprocal(_tmp_sum_on_rows)\n",
    "norm = np.diagflat( _tmp_sum_on_rows_recip )\n",
    "\n",
    "T = np.matmul( _tmp, norm )\n",
    "\n",
    "num_nodes = len(G.nodes())\n",
    "alpha = 2.0/(2.0 + mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = args.num_steps\n",
    "convcheckfreq = args.check_every\n",
    "conv_thr = args.conv_thr #1e-06\n",
    "\n",
    "repeatz = args.repeats\n",
    "\n",
    "print(\"Alpha = %.3f\" %(alpha))\n",
    "\n",
    "def rw(sn): #signed node\n",
    "    n0 = sn\n",
    "    print(\"RW from signed node %i started.\" % n0)\n",
    "    rw_visits = np.zeros( num_nodes )\n",
    "    last_rw_visits = np.copy(rw_visits)\n",
    "\n",
    "    nodes = range(num_nodes)\n",
    "\n",
    "    for s0 in range(1,steps+1):\n",
    "\n",
    "        #start from\n",
    "        #go back to labeled node? prob = 1-alpha; prob to trans = alpha\n",
    "        if (uniform() < alpha): #\n",
    "            trans_probs = T[:,n0].view(np.ndarray).flatten() \n",
    "            trans_to = int( choice(nodes, 1, p = trans_probs) )\n",
    "            n0 = trans_to\n",
    "        else:\n",
    "            n0 = sn\n",
    "\n",
    "        rw_visits[n0] += 1\n",
    "\n",
    "        if (s0 % convcheckfreq == 0):\n",
    "            diff_rw_visits = ((rw_visits/s0 - last_rw_visits/s0)**2).sum()\n",
    "            print(\"[sn %d] At step %d, diff %.10f\" %(sn, s0, diff_rw_visits) )\n",
    "            if ((diff_rw_visits) < conv_thr):# and np.all(rw_visits != last_rw_visits)):\n",
    "                print(\"Converged at <%.10f\" % conv_thr)\n",
    "                break\n",
    "            last_rw_visits = np.copy(rw_visits)\n",
    "\n",
    "    return rw_visits/s0, s0\n",
    "\n",
    "#output\n",
    "seed_nodes_seq = []\n",
    "communities_seq = []\n",
    "\n",
    "for _sn, _comm in labs*repeatz:\n",
    "    seed_nodes_seq.append(_sn)\n",
    "    communities_seq.append(_comm)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as pool:\n",
    "        res = pool.map(rw, seed_nodes_seq)\n",
    "\n",
    "#consolidate results\n",
    "all_prob_distr = defaultdict(list) #k = community : v = seednode, steps, probdistr\n",
    "\n",
    "for r, (prob, steps) in enumerate(res):\n",
    "    print(\"RW %d (seed node %d, comm %d) run for %d steps\" % (r, seed_nodes_seq[r], communities_seq[r], steps))\n",
    "    all_prob_distr[communities_seq[r]].append((seed_nodes_seq[r], steps, prob))\n",
    "    \n",
    "#save as numpy, just in case\n",
    "with open('probs.npy', 'wb') as f:\n",
    "    np.save(f, all_prob_distr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_keys = all_prob_distr.keys()\n",
    "\n",
    "#list of lists: [community][repetition]\n",
    "probs_vectors_by_comm_by_repeat = { ck: [all_prob_distr[ck][rep][2] for rep in range(repeatz)] for ck in comms_keys}\n",
    "probs_weights_by_comm_by_repeat = { ck: [all_prob_distr[ck][rep][1] for rep in range(repeatz)] for ck in comms_keys}\n",
    "\n",
    "#average by community (need to weight by steps before convergence? I'd say yes)\n",
    "avg_probs = {ck: np.average(probs_vectors_by_comm_by_repeat[ck], axis = 0,\n",
    "                            weights = probs_weights_by_comm_by_repeat[ck]) for ck in comms_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FN = 'RW_avg_probs.csv'\n",
    "pd.DataFrame(avg_probs).to_csv(OUT_FN)\n",
    "print(\"Averaged RW probabilities dumped to file %s.\" % OUT_FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
